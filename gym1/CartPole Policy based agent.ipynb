{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cc73b86",
   "metadata": {},
   "source": [
    "# Reinforcement Learning using DL and Keras RL\n",
    "\n",
    "In this document we will try to train an agent for the Open AI gym enviorment called 'CartPole-v0' and try to acive the perfect score of *200* \n",
    "\n",
    "we will try to divide the Notebook in different sections so that its easy to refference it again.\n",
    "\n",
    "By:- Yash Tripathi\n",
    "\n",
    "Followed the video :- [The link to the Refference Video](https://youtu.be/cO5g5qLrLSo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc02a3f",
   "metadata": {},
   "source": [
    "## Importing the Libraries and creating the Gym Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd90503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "354c067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager().update('notebook', {'limit_output': 1000000000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d56e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69a3fbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78683647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes = 10\n",
    "# for episode in range(1,episodes + 1):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     score = 0\n",
    "    \n",
    "#     while not done:\n",
    "#         env.render()\n",
    "#         time.sleep(0.1)\n",
    "#         action = random.choice([0,1])\n",
    "#         n_state,reward,done,info = env.step(action)\n",
    "#         score += reward\n",
    "        \n",
    "#     print(\"For Episode:{} Score:{}\".format(episode,score))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022a3ab2",
   "metadata": {},
   "source": [
    "## Creating The ML Model for the Reinforcement agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9663d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed51ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states,action):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,states)))\n",
    "    model.add(Dense(24,activation=\"relu\"))\n",
    "    model.add(Dense(24,activation=\"relu\"))       \n",
    "    model.add(Dense(actions,activation=\"linear\"))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "318e404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83f8ec00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7356875",
   "metadata": {},
   "source": [
    "## Creating the DQN RL Agent and Tranning it\n",
    "\n",
    "In this section I have also imported the Previously generated modle so that the tranning is faster. You can either comment that section so you wont have the previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8b68d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0fc0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model,actions):\n",
    "    \n",
    "    policy = BoltzmannQPolicy()\n",
    "    \n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    \n",
    "    dqn = DQNAgent(model=model , memory=memory , policy= policy, nb_actions=actions,nb_steps_warmup=1000,target_model_update=1e-2)\n",
    "    \n",
    "    return dqn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb4e3f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Sequential' object has no attribute '_compile_time_distribution_strategy'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-09 19:06:54.272671: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "## There is a weird bug where you will have to first complie it and fail Intentionally \n",
    "## Then you will have to delet the existing DL model to then create it again using the new changes\n",
    "## hence the loop below is used \n",
    "\n",
    "ruroRan = True ## Shows if the first run of the compiler is completed and we have the final model\n",
    "\n",
    "while(ruroRan):\n",
    "    try:\n",
    "        dqn = build_agent(model,actions)\n",
    "        dqn.compile(Adam(learning_rate=1e-3),metrics=['mae'])\n",
    "        ruroRan = False\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        del model\n",
    "        model = build_model(states,actions)\n",
    "        ruroRan = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef20cc76",
   "metadata": {},
   "source": [
    "WE load the previous weights in this cell,\n",
    "\n",
    "Comment it if you do not want to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8334c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights(\"carpole1.h5f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57d74ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashtripathi/miniforge3/envs/test1/lib/python3.9/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 17s 2ms/step - reward: 1.0000\n",
      "done, took 17.474 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x177b38310>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tranning the model with the specfied steps in nb_steps\n",
    "dqn.fit(env,nb_steps=10000,visualize=False,verbose = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad77f2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "Episode 6: reward: 200.000, steps: 200\n",
      "Episode 7: reward: 200.000, steps: 200\n",
      "Episode 8: reward: 200.000, steps: 200\n",
      "Episode 9: reward: 200.000, steps: 200\n",
      "Episode 10: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x177d6a100>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env,nb_episodes=10,visualize= False,verbose = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60348389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 18s 2ms/step - reward: 1.0000\n",
      "done, took 17.824 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x177d6a070>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env,nb_steps=10000,visualize=False,verbose = 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13e09f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "Episode 6: reward: 200.000, steps: 200\n",
      "Episode 7: reward: 200.000, steps: 200\n",
      "Episode 8: reward: 200.000, steps: 200\n",
      "Episode 9: reward: 200.000, steps: 200\n",
      "Episode 10: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x177d6a880>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env,nb_episodes=10,visualize= False,verbose = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46f7d41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "Episode 6: reward: 200.000, steps: 200\n",
      "Episode 7: reward: 200.000, steps: 200\n",
      "Episode 8: reward: 200.000, steps: 200\n",
      "Episode 9: reward: 200.000, steps: 200\n",
      "Episode 10: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x177d6a430>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env,nb_episodes=10,visualize=True,verbose = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b307e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights(\"carpole1.h5f\",overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d66e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "479.9864196777344px",
    "left": "3.790771245956421px",
    "right": "20px",
    "top": "34.00000762939453px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
